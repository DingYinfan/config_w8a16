[	
    {
        "model_name": "llama2_7b_w8a16_wo_kv",
        "args": "--vllm-path=llama2_7b_w8a16_wo_kv --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs --tensor-parallel-size=1 --model-kwargs dtype=float16 quantization=w8a16"
    },
	
    {
        "model_name": "llama2_7b_w8a16_wo_kv-2cards",
        "args": "--vllm-path=llama2_7b_w8a16_wo_kv --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs --tensor-parallel-size=2 --model-kwargs dtype=float16 quantization=w8a16"
    },
	
    {
        "model_name": "llama2_13b_w8a16_wo_kv",
        "args": "--vllm-path=llama2_13b_w8a16_wo_kv --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs --tensor-parallel-size=1 --model-kwargs dtype=float16 quantization=w8a16"
    },
	
    {
        "model_name": "llama2_13b_w8a16_wo_kv-2cards",
        "args": "--vllm-path=llama2_13b_w8a16_wo_kv --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs --tensor-parallel-size=2 --model-kwargs dtype=float16 quantization=w8a16"
    },
	
    {
        "model_name": "llama2_70b_w8a16_wo_kv",
        "args": "--vllm-path=llama2_70b_w8a16_wo_kv --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs --tensor-parallel-size=2 --model-kwargs dtype=float16 quantization=w8a16"
    },
	
    {
        "model_name": "llama3_8b_w8a16_wo_kv",
        "args": "--vllm-path=llama3_8b_w8a16_wo_kv --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs --tensor-parallel-size=1 --model-kwargs dtype=float16 quantization=w8a16"
    },
	
    {
        "model_name": "llama3_70b_w8a16_wo_kv",
        "args": "--vllm-path=llama3_70b_w8a16_wo_kv --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs --tensor-parallel-size=2 --model-kwargs max_model_len=4096 quantization=w8a16 dtype=half gpu_memory_utilization=0.945"
    },
    
    {
        "model_name": "llama3_70b_w8a16_groupsize_64",
        "args": "--vllm-path=llama3_70b_w8a16_groupsize_64 --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --work-dir=outputs --max-out-len=10 --batch-size=1 --tensor-parallel-size=8 --model-kwargs dtype=float16 max_model_len=8192 quantization=w8a16"
    }
]