[
    {
        "model_name": "mixtral_8x7b_w8a16",
        "args": "--model=mixtral_8x7b_w8a16 --device=cuda --dtype=float16 --max-model-len 4096 --quantization=w8a16 --dataset=./llm_samples_new/mistral-7b-v0.1/mixtral_8x7b_v0.1-w8a16.json --tensor-parallel-size=4 --save-output=./output/mistral-7b-v0.1/mixtral_8x7b_v0.1-w8a16.json"
    }
]